---
title: 'ECON-613 HW #4'
author: "Peter Kim"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r, include = FALSE}
library(tidyverse)
library(knitr)
library(readr)
library(ggplot2)
```

# Exercise 1 Data

```{r}
#Set Seed
set.seed(1)

#Read Data
data = read_csv("~/ECON-613/Assignment #4/HW #4/Koop-Tobias.csv")

#Identifying Unique Person ID
uniq_ID = unique(data$PERSONID) %>% length()

#Generating 5 Random Indices
ind = sample(x = 1:uniq_ID, size = 5, replace = FALSE)

#Representing the Panel Dimension of Wages for 5 Randomly Selected Individuals
rand5 = data %>%
  filter(PERSONID %in% ind) %>%
  select(PERSONID, TIMETRND, LOGWAGE)

rand5.1 = rand5 %>% filter(PERSONID == unique(rand5$PERSONID)[1])
rand5.2 = rand5 %>% filter(PERSONID == unique(rand5$PERSONID)[2])
rand5.3 = rand5 %>% filter(PERSONID == unique(rand5$PERSONID)[3])
rand5.4 = rand5 %>% filter(PERSONID == unique(rand5$PERSONID)[4])
rand5.5 = rand5 %>% filter(PERSONID == unique(rand5$PERSONID)[5])

ggplot() + 
  geom_line(data = rand5.1, 
            mapping = aes(x = TIMETRND, y = LOGWAGE, color = "439")) +
  geom_line(data = rand5.2, 
            mapping =  aes(x = TIMETRND, y = LOGWAGE, color = "579")) +
  geom_line(data = rand5.3, 
            mapping =  aes(x = TIMETRND, y = LOGWAGE, color = "811")) +
  geom_line(data = rand5.4, 
            mapping =  aes(x = TIMETRND, y = LOGWAGE, color = "1247")) +
  geom_line(data = rand5.5, 
            mapping =  aes(x = TIMETRND, y = LOGWAGE, color = "1976")) +
  scale_color_manual(
    name = "Person ID",
    values = c("439" = "red",
               "579" = "blue",
               "811" = "green",
               "1247" = "black",
               "1976" = "orange")
  ) +
  labs(
    x = "Time Trend",
    y = "Wage (in log units)",
    title = "5 Randomly Selected Individuals' Wages in Time"
  ) +
  theme_bw()

#Alternative Way to Represent the Panel Dimension of Wages for 5 Randomly Selected Individuals
panel5 = data %>% 
  filter(PERSONID %in% unique(rand5$PERSONID)) %>%
  group_by(PERSONID) %>%
  summarise(Size = n())

kable(panel5, 
      caption = "Table of Number of Time 5 Randomly Selected Individuals Showed Up in Data")
```

Table of Number of Time 5 Randomly Selected Individuals Showed Up in Data shows that the dataset is unbalanced panel data.

# Exercise 2 Random Effects

```{r}
#Implementing Linear Regresson 
randlm = lme4::lmer(LOGWAGE ~ EDUC + POTEXPER + (1|PERSONID), data = data)

randlm_coef = randlm@beta %>% as.data.frame()
colnames(randlm_coef) = c("OLS Coefficients")
rownames(randlm_coef) = c("Intercept", "EDUC", "POTEXPER")

kable(randlm_coef, digits = 4, 
      caption = "Table of OLS Coefficients for Random Effects Model")
```

### Comment

$\hat{\beta}_{intercept} = 0.7942$: When education and potential experience are 0, log wage will be 0.7942. This quantity, however, is meaningless since no individuals are legally permitted to have 0 units of education. \newline

$\hat{\beta}_{EDUC} = 0.0939$: Ceteris paribus, a one unit increase in education will increase log wage by 0.0939 on average. \newline

$\hat{\beta}_{POTEXPER} = 0.0374$: Ceteris paribus, a one unit increase in potential experience will increase log wage by 0.0374 on average. \newline


# Exercise 3

## Between Estimator

We calculate between estimators here and save them as between_coef.

```{r}
between_data = data %>% 
  select(PERSONID, EDUC, LOGWAGE, POTEXPER, TIMETRND) %>% 
  group_by(PERSONID) %>% 
  summarise(mean_LOGWAGE = mean(LOGWAGE),
            mean_EDUC = mean(EDUC),
            mean_POTEXPER = mean(POTEXPER))

between_coef = 
  lm(mean_LOGWAGE ~ mean_EDUC + mean_POTEXPER, data = between_data) %>% 
  coefficients() %>%
  as.data.frame()
```

## Within Estimator

We calculate within estimators here and save them as within_coef.

```{r}
#Creating Initial Within Data
within_data = left_join(data, between_data, by = "PERSONID")

#Updating Within Data - Adding Columns of Y - Y_bar and X - X_bar
within_data2 = within_data %>%
  mutate(bet_resp = LOGWAGE - mean_LOGWAGE,
         bet_EDUC = EDUC - mean_EDUC,
         bet_POTEXPER = POTEXPER - mean_POTEXPER) %>%
  select(PERSONID, bet_resp, bet_EDUC, bet_POTEXPER)

#Calculating Within Coefficients
within_coef = 
  lm(bet_resp ~ bet_EDUC + bet_POTEXPER - 1, data = within_data2) %>%
  coefficients() %>%
  as.data.frame()
```

## First Time Difference Estimator

We calculate first time difference estimators here and save them as first_coef.

```{r}
#Creating First Difference Data
first_data = 
  data %>% 
  group_by(PERSONID) %>%
  mutate(LOGWAGE_Diff = LOGWAGE - lag(LOGWAGE),
           EDUC_Diff = EDUC - lag(EDUC),
           POTEXPER_Diff = POTEXPER - lag(POTEXPER)) %>%
  select(PERSONID, LOGWAGE_Diff, EDUC_Diff, POTEXPER_Diff) %>%
  na.omit()

first_coef = 
  lm(LOGWAGE_Diff ~ EDUC_Diff + POTEXPER_Diff - 1, data =  first_data) %>% 
  coefficients() %>%
  as.data.frame()
```

Here, we create a table of $\hat{\beta}_{EDUC}$ and $\hat{\beta}_{POTEXPER}$

```{r}
coef_data = data.frame(
  c(between_coef %>% unlist() %>% as.numeric()),
  c(NA, within_coef %>% unlist() %>% as.numeric()),
  c(NA, first_coef %>% unlist() %>% as.numeric)
)
colnames(coef_data) = c("Between", "Within", "First")
rownames(coef_data) = c("Intercept", "Education", "POTEXPER")

kable(coef_data, digits = 4, 
      caption = "Coefficients Under Different Models")
```

Comparison of $\hat{\beta}_{education}$'s:

We observe that $\hat{\beta}_{between}$, $\hat{\beta}_{within}$, and $\hat{\beta}_{first}$ are all positive. Each model believes that a unit increase in education will increase wage (or log wage). The "within" model has the largest magnitude, while the "first-difference" model has the smallest magnitude.

Comparison of $\hat{\beta}_{POTEXPER}$'s:

Similar to education, we observe that $\hat{\beta}_{between}$, $\hat{\beta}_{within}$, and $\hat{\beta}_{first}$ are all positive. Each model believes that a unit increase in potential experience will increase wage (or log wage).. The "within" model has the largest magnitude, while the "between" model has the smallest magnitude.

# Exercise 4

```{r}
#Randomly Selecting 100 Indices
ind100 = sample(x = 1:uniq_ID, size = 100, replace = FALSE)

#Extracting Dataframe Associated with 100 Indices
data_ex4 = data %>% filter(PERSONID %in% ind100)

x_ex4 = data_ex4 %>% select(EDUC, POTEXPER)
y_ex4 = data_ex4 %>% select(LOGWAGE)

#Writing ML
normal_ML = function(parm){
  
  #Converting Y into Matrix
  Y = y_ex4 %>% as.matrix()
  
  #Adding Intercept Column
  X = cbind(1, x_ex4) %>% as.matrix()
  
  #Calculating XB
  XB = X %*% parm[1:3]
  
  #Conditional Variance
  s = (parm[4])^2
  
  #Calculating Likelihood
  lik =
    prod( (2 * pi * cond_var)^(1/2) * exp( -(Y - XB)^2 / (2 * s)) )
  
  #Returning the Likelihood
  return(lik)
  
}

#Writing Log ML
normal_logML = function(parm){
  
  #Converting Y into Matrix
  Y = y_ex4 %>% as.matrix()
  
  #Intercept Column
  X_inter = matrix(1, nrow = nrow(x_ex4), ncol = 100)
  
  #Adding Intercept Column
  X = cbind(X_inter, x_ex4) %>% as.matrix()
  
  #Calculating XB
  XB = X %*% parm[1:102]
  
  #Variance
  var = (parm[103])^2
  
  #Calculating Log Likelihood
  log_lik = 
    sum( -(1/2) * ( log(2 * pi) + log(var) ) - 
           (1/2) * (Y - XB)^2 / var )
  
  #Returning the Negative of the Log Likelihood
  return(-log_lik)
  
}


```

Here, we are running a regression to estimate individual fixed effects on the invariant variables.

```{r}
colnames(data_ex4)
lm(LOGWAGE ~ ABILITY + MOTHERED + FATHERED + BRKNHOME + SIBLINGS, data = data_ex4)
```

