---
title: 'ECON-613 HW #3'
author: "Peter Kim"
date: "`r Sys.Date()`"
output:
  pdf_document: default
  html_document: default
---

```{r, include = FALSE}
install.packages("bayesm")
install.packages("mlogit")
library(tidyverse)
library(bayesm)
library(knitr)
library(mlogit)
```

# Exercise1

```{r}
#Reading Data
data(margarine)

#Subsetting Data
mar_choicePrice = margarine$choicePrice %>% as.data.frame()
mar_demos = margarine$demos %>% as.data.frame()

#Average and Spread by Columns
avg_col = mar_choicePrice[, 3:12] %>% colMeans() 
spread_col = mar_choicePrice[, 3:12] %>% apply(MARGIN = 2, FUN = sd) 

#Dataframe of Average and Spread by Columns
avg_spread_col_df = data.frame(
  avg_col,
  spread_col
)
colnames(avg_spread_col_df) = c("average", "spread")

#Table of Average and Spread by Columns
kable(avg_spread_col_df, digits = 4, 
      caption = "Table of Average and Spread by Column")

#Identifying Unique Brands
brands = sub("_.*", "", colnames(mar_choicePrice))[3:12] %>% unique()

#Indices of Each Brand
index1 = grepl(brands[1], colnames(mar_choicePrice)) %>% which()
index2 = grepl(brands[2], colnames(mar_choicePrice)) %>% which()
index3 = grepl(brands[3], colnames(mar_choicePrice)) %>% which()
index4 = grepl(brands[4], colnames(mar_choicePrice)) %>% which()
index5 = grepl(brands[5], colnames(mar_choicePrice)) %>% which()
index6 = grepl(brands[6], colnames(mar_choicePrice)) %>% which()
index7 = grepl(brands[7], colnames(mar_choicePrice)) %>% which()

#Average and Spread by Brand
b1 = mar_choicePrice[, index1] %>% gather(key) %>% .[, 2] 
b2 = mar_choicePrice[, index2] 
b3 = mar_choicePrice[, index3] %>% gather(key) %>% .[, 2] 
b4 = mar_choicePrice[, index4] %>% gather(key) %>% .[, 2] 
b5 = mar_choicePrice[, index5] 
b6 = mar_choicePrice[, index6] 
b7 = mar_choicePrice[, index7] 

#Table of Average and Spread by Brand
brand_avg_sd_df = data.frame(
  c(brands[1:7]),
  c(mean(b1), mean(b2), mean(b3), mean(b4), mean(b5), mean(b6), mean(b7)),
  c(sd(b1), sd(b2), sd(b3), sd(b4), sd(b5), sd(b6), sd(b7))
)
colnames(brand_avg_sd_df) = c("Brand", "Mean", "Sd")

kable(brand_avg_sd_df, digits = 4, 
      caption = "Table of Average and Spread by Brand")

#Average and Spread by Stick and Tubs
stick_avg_spread = select(mar_choicePrice, contains("Stk")) %>% 
  gather(key = "stk", value = vals) %>%
  summarise(mean = mean(vals), var = sd(vals)^2)
rownames(stick_avg_spread) = "Stick"

tub_avg_spread = select(mar_choicePrice, contains("Tub")) %>% 
  gather(key = "tub", value = vals) %>%
  summarise(mean = mean(vals), var = var(vals))
rownames(tub_avg_spread) = "Tub"

avg_spread_char_df = rbind(stick_avg_spread, tub_avg_spread)

kable(avg_spread_char_df, digits = 4, 
      caption = "Table of Average and Spread (Var) by Stick and Tubs")

#Market Share
choice_total = table(mar_choicePrice$choice) %>% sum()
market_share = (table(mar_choicePrice$choice) / choice_total) %>% 
  as.data.frame()

#Removing Var1
market_share = market_share %>%
  select(Freq)

#Providing Appropriate Colnames and Rownames
colnames(market_share) = "Market Share"
rownames(market_share) = colnames(mar_choicePrice)[3:12]

#Table of Market Share
kable(market_share, digits = 4, 
      caption = "Table of Market Share Based on Products")

#Finding Indices of Sticks and Tubs
stk_idx = colnames(mar_choicePrice[3:12]) %>% 
  ends_with(match = "stk", ignore.case = TRUE)
tub_idx = colnames(mar_choicePrice[3:12]) %>%
  ends_with(match = "tub", ignore.case = TRUE)

#Calculating the Total Numbers of Sticks and Tubs
stk_tot = table(mar_choicePrice$choice)[stk_idx] %>% sum()
tub_tot = table(mar_choicePrice$choice)[tub_idx] %>% sum()

#Calculating Market Shares Based on Sticks and Tubs
market_share_char = data.frame(
  c(stk_tot / dim(mar_choicePrice)[1], tub_tot / dim(mar_choicePrice)[1])
)
colnames(market_share_char) = "Market Share"
rownames(market_share_char) = c("Sticks", "Tubs")

#Table of Market Shares Based on Sticks and Tubs
kable(market_share_char, digits = 4, 
      caption = "Table of Market Shares Based on Sticks vs. Tubs")

#Defining Table of Choices
tb = table(mar_choicePrice$choice)

#Calculating Market Share of Each Brand
mark1 = tb[index1 - 2] %>% sum() / 4470
mark2 = tb[index2 - 2] %>% sum() / 4470
mark3 = tb[index3 - 2] %>% sum() / 4470
mark4 = tb[index4 - 2] %>% sum() / 4470
mark5 = tb[index5 - 2] %>% sum() / 4470
mark6 = tb[index6 - 2] %>% sum() / 4470
mark7 = tb[index7 - 2] %>% sum() / 4470

#Dataframe of Frequency of Each Brand
brand_df = data.frame(
  c(brands[1:7]),
  c(mark1, mark2, mark3, mark4, mark5, mark6, mark7)
)
colnames(brand_df) = c("Brands", "Marketshare")

kable(brand_df, digits = 4, caption = "Market Share by Brand")


#Counting Choices Based on Income
choices_Income_df = left_join(x = mar_choicePrice, y = mar_demos, 
                              by = "hhid") %>% select(choice, Income) %>%
  group_by(choice, Income) %>% 
  tally() %>% 
  arrange(Income) %>% 
  spread(key = Income, value = n)

#Replacing NA's with 0
choices_Income_df[is.na(choices_Income_df)] = 0 

#Calculating Market Share by Income
Income_market_df = data.frame(
  colnames(mar_choicePrice[, 3:12]),
  apply(choices_Income_df[, 2:15], 2, function(x) x / sum(x))
)
colnames(Income_market_df) = 
  c("Brands", mar_demos$Income %>% unique() %>% sort())

kable(Income_market_df[, 1:6], digits = 4, 
      caption = "Mapping Between Observed Attributes and Choices")

kable(Income_market_df[, 7:15], digits = 4, 
      caption = "Mapping Between Observed Attributes and Choices")
```

## Comment

For mapping between observed attributes and choices, I wanted to know which product has been purchased the most based on each income category. 

# Exercise2

For this exercise, I propose to use a conditional logit model. \newline

According to the class slide, for conditional logit model,
$$
p_{ij} = \frac{exp(x_{ij} \beta)}{\sum_{l=1}^{m} exp(x_{il} \beta)}
$$  

According to page 496 of textbook, 

$$
\begin{aligned}
L(\beta \mid x_{ij}, y_{ij}) = \Pi_{i=1}^{n} \Pi_{j=1}^{m} p_{ij}^{y_{ij}}
\end{aligned}
$$

But before we calculate the likelihood, we have to calculate the indicator variable $y_{ij}$ for $i \in \{1, ..., 4470 \}$ and $j \in \{1, ..., 10 \}$. $y$ represents column "choice" in the dataset.

$$
y_{ij} = 
\begin{cases} 
      1 & y = j \\
      0 & y \neq j 
\end{cases}
$$

```{r}
#Allocating Memory for Matrix y
y = matrix(0, nrow = dim(mar_choicePrice)[1], 
           ncol = dim(mar_choicePrice[, 3:12])[2])

#Calculating y
for( j in seq(1, 10) ){
  for( i in 1:dim(mar_choicePrice)[1] ){
    
    if(mar_choicePrice$choice[i] == j){
        y[i, j] = 1
    }
    
  }
}
```

Now, we implement the conditional logit likelihood.

```{r}
#Condtional Logit Likelihood Function
cond_logit_lik = function(beta){
  
  #Intercept Matrix
  alpha = matrix( rep( c(0, beta[1:9]), each = n ), nrow = n, ncol = 10)
  
  #Non-Intercept Matrix
  xbeta = as.matrix( mar_choicePrice[, 3:12] ) * beta[10]
  
  #Calculating XB
  XB = alpha + xbeta
  
  #Calculating Numerator
  N = exp(XB)
  
  #Calculating Denominator
  D = exp(XB) %>% rowSums()
  
  #Calculating Probability p
  p_ij = N / D
  
  #Calculating Likelihood
  likelihood = prod( p_ij^(y) )
  
  return(likelihood)
  
}
```

The log likelihood of the conditional logit model is the following. Note that negative of the log likelihood value is returned.

```{r}
#Conditional Logit Log Likelihood
cond_logit_log_lik = function(beta){
  
  #Intercept Matrix
  alpha = matrix( rep( c(0, beta[1:9]), each = n ), nrow = n, ncol = 10)
  
  #Non-Intercept Matrix
  xbeta = as.matrix( mar_choicePrice[, 3:12] ) * beta[10]
  
  #Calculating XB
  XB = alpha + xbeta
  
  #Calculating Numerator
  N = exp(XB)
  
  #Calculating Denominator
  D = exp(XB) %>% rowSums()
  
  #Calculating Probability p
  p_ij = N / D
  
  #Calculating log Likelihood
  log_lik = sum( y * log( p_ij ) )
    
  return(-log_lik)
  
}
```

Since the likelihood and loglikelihood functions are defined above, we now optimize the model and present the results. Note that $\hat{\alpha}$'s represent intercepts.

```{r, warning = FALSE}
#Dimension Calculation for Following Intercept Matrix
n = nrow( mar_choicePrice )
  
#Initializing Parameters
b = rep(-1.8, 10)

#Optimizing Log Likelihood Function
opt = optim(par = b, fn = cond_logit_log_lik)

cond_opt_df = data.frame(
  opt$par
  )
colnames(cond_opt_df) = c("Optimized Coefficient")
rownames(cond_opt_df) = c("$\\hat{\\alpha}_{1}$", "$\\hat{\\alpha}_{2}$",
                     "$\\hat{\\alpha}_{3}$", "$\\hat{\\alpha}_{4}$",
                     "$\\hat{\\alpha}_{5}$", "$\\hat{\\alpha}_{6}$",
                     "$\\hat{\\alpha}_{7}$", "$\\hat{\\alpha}_{8}$",
                     "$\\hat{\\alpha}_{9}$", "$\\hat{\\beta}_{price}$")

kable(cond_opt_df, digits = 4, caption = "Price Coefficient Under Conditional Logit")
```

## Interpretation

According to the table, $\hat{\beta}_{price} = -6.3206$. This means that the likelihood of product being purchased decreases as the price increases.

# Exercise3

For this exercise, I propose to use multinomial logit model.

According to page 494 of the textbook, for multinomial logit model,

$$
p_{ij} = \frac{exp(\alpha_{j} + \beta_{Ij} I_{i})}{\sum_{k=1}^{m} exp(\alpha_{k} + \beta_{Ik} I_{i})}
$$

where $I$ denotes income. \newline

According to page 496 of textbook, 

$$
\begin{aligned}
L(\beta \mid x_{ij}, y_{ij}) = \Pi_{i=1}^{n} \Pi_{j=1}^{m} p_{ij}^{y_{ij}}
\end{aligned}
$$

Since we have calculated y in exercise 2, we proceed to implementing the multinomial logit likelihood. We also merge choicePrice data frame and demos data frame here.

```{r}
#Merging Data to Put Choice and Income in One Data Frame
merged_data = left_join(x = mar_choicePrice, y = mar_demos, by = "hhid") 

#Choice and Income Data Frame
choice_I_df = merged_data %>%
  select(choice, Income) %>%
  arrange(Income)

#Defining Variable I
I = choice_I_df$Income
```


```{r}
#Multinomial Logit Likelihood
multi_logit_lik = function(beta){
  
  #Intercept Matrix
  alpha = matrix( rep( beta[1:10], each = n ), nrow = n, ncol = 10 ) 
  
  #Non-Intercept Matrix
  xbeta = matrix( rep( beta[11:20], each = n), nrow = n, ncol = 10 ) * I
  
  #Calculating XB
  XB = alpha + xbeta
  
  #Calculating Numerator
  N = exp(XB)
  
  #Calculating Denominator
  D = exp(XB) %>% rowSums()
  
  #Calculating Probability p
  p_ij = N / D
  
  #Calculating Likelihood
  likelihood = prod( p_ij^(y) )
  
  return(likelihood)
  
}
```

We now calculate the log likelihood.

```{r}
#Multinomial Logit Log Likelihood
multi_logit_log_lik = function(betas){
  
  #Intercept Matrix
  alpha = matrix( rep( betas[1:10], each = n), nrow = n, ncol = 10 )
  
  #Non-Intercept Matrix
  xbeta = matrix( rep( betas[11:20], each = n), nrow = n, ncol = 10 ) * I
  
  #Calculating XB
  XB = alpha + xbeta
  
  #Calculating Numerator
  N = exp(XB)
  
  #Calculating Denominator
  D = exp(XB) %>% rowSums()
  
  #Calculating Probability p
  p_ij = N / D
  
  #Calculating log Likelihood
  log_logit = sum( y*log( p_ij ) )
  
  return(-log_logit)
  
}
```

Since the likelihood and loglikelihood functions are defined above, we now optimize the model. 

```{r, warning = FALSE}
betas = c(0, -0.5, -2.5, -1.5, -1.5, -4, -1.5, -3, -2.5, -4, 
         rep(0, 10))

I = choice_I_df$Income

#Optimizing Log Likelihood Function
multi_opt = optim(par = betas, fn = multi_logit_log_lik)

multi_opt_df = data.frame(
  multi_opt$par[1:10],
  multi_opt$par[11:20]
  )
colnames(multi_opt_df) = c("Optimized $\\hat{\\alpha}$", "Optimzed $\\hat{\\beta}$")
rownames(multi_opt_df) = c("Reference", seq(2, 10))

kable(multi_opt_df, digits = 4, caption = "Coefficients Under Multinomial Logit")
```

## Interpretation
$\beta_{Income_{2}}$: It is more likely for an individual to choose choice 1 than choice 2 if his or her income increases. \newline

$\beta_{Income_{3}}$: It is more likely for an individual to choose choice 1 than choice 3 if his or her income increases. \newline

$\beta_{Income_{4}}$: It is more likely for an individual to choose choice 4 than choice 1 if his or her income increases. \newline

$\beta_{Income_{5}}$: It is more likely for an individual to choose choice 5 than choice 1 if his or her income increases. \newline

$\beta_{Income_{6}}$: It is more likely for an individual to choose choice 6 than choice 1 if his or her income increases. \newline

$\beta_{Income_{7}}$: It is more likely for an individual to choose choice 7 than choice 1 if his or her income increases. \newline

$\beta_{Income_{8}}$: It is more likely for an individual to choose choice 1 than choice 8 if his or her income increases. \newline

$\beta_{Income_{9}}$: It is more likely for an individual to choose choice 1 than choice 9 if his or her income increases. \newline

$\beta_{Income_{10}}$: It is more likely for an individual to choose choice 10 than choice 1 if his or her income increases. \newline



# Exercise4

For conditional logit, the marginal effect is computed as the following.

```{r}
#Conditional Logit Log Likelihood
cond_logit_log_lik2 = function(beta){
  
  #Intercept Matrix
  alpha = matrix( rep( c(0, beta[1:9]), each = n ), nrow = n, ncol = 10)
  
  #Non-Intercept Matrix
  xbeta = as.matrix( mar_choicePrice[, 3:12] ) * beta[10]
  
  #Calculating XB
  XB = alpha + xbeta
  
  #Calculating Numerator
  N = exp(XB)
  
  #Calculating Denominator
  D = exp(XB) %>% rowSums()
  
  #Calculating Probability p
  p_ij = N / D
  
  #Calculating log Likelihood
  log_lik = sum( y * log( p_ij ) )
    
  return(p_ij)
  
}

#Saving P from Conditional Logit Log Likelihood
P = cond_logit_log_lik2(cond_opt_df$`Optimized Coefficient`)

#Saving Price Beta from Estimated Coefficient
beta = cond_opt_df$`Optimized Coefficient`[10]

#Allocating Memory for Conditional Marginal
conditional_marginal = matrix(NA, nrow = 10, ncol = 10)

#Calculating Average Marginal Effect
for( j in 1:10 ){
  for( k in 1:10 ){
    
    delta = ifelse(j == k, 1, 0)
    conditional_marginal[j, k] = mean(P[, j] * (delta - P[, k]) * beta)
    
  }
}

cond_marg_df = conditional_marginal %>% as.data.frame()

kable(cond_marg_df[,1:5], digits = 4, caption = 
        "Average Marginal Effect Under Conditional")

kable(cond_marg_df[,6:10], digits = 4, caption = 
        "Average Marginal Effect Under Conditional")

```

## Interpretation
Each increase in price decreases the probability of choosing jth choice but increases the probability of choosing non-jth choice.

For multinomial logit, the marginal effect is calculated as the following.

```{r}
#Multinomial Logit Log Likelihood
multi_logit_log_lik2 = function(betas){
  
  #Intercept Matrix
  alpha = matrix( rep( betas[1:10], each = n), nrow = n, ncol = 10 )
  
  #Non-Intercept Matrix
  xbeta = matrix( rep( betas[11:20], each = n), nrow = n, ncol = 10 ) * I
  
  #Calculating XB
  XB = alpha + xbeta
  
  #Calculating Numerator
  N = exp(XB)
  
  #Calculating Denominator
  D = exp(XB) %>% rowSums()
  
  #Calculating Probability p
  p_ij = N / D
  
  #Calculating log Likelihood
  log_logit = sum( y*log( p_ij ) )
  
  return(p_ij)
  
}

#Probability from Multinomial
multi_P = multi_logit_log_lik2(multi_opt$par)

#Betas
multi_beta = multi_opt$par[11:20]

#Calculating Beta-Bar
beta_bar = multi_P %*% (multi_beta)

#Allocating Memory for Multinomial Marginal Effect
multi_marginal = rep(NA, 10)

#Calculating Multinomial Marginal Effect
for(j in 1:10){
  multi_marginal[j] = mean(multi_P[, j] * (multi_opt$par[11:20] - beta_bar))
}

multi_mar_df = multi_marginal %>% as.data.frame()

kable(multi_mar_df, digits = 4, col.names = "Average Marginal Effect",
      caption = "Average Marginal Effect Under Multinomial Logit")
```

## Interpretaion

Each one-unit increase in income changes the probability of selecting jth choice by the given magnitude.


# Exercise5

The following is the mixed logit log likelihood function.

```{r}
mixed_logit_log_lik = function(beta){
  
  #Intercept Matrix
  alpha = matrix( rep( beta[1:10], each = n ), nrow = n, ncol = 10)
  
  #Non-Intercept Matrix
  xbeta_I = matrix( rep( beta[11:20], each = n), nrow = n, ncol = 10 ) * I
  xbeta_p = as.matrix( mar_choicePrice[, 3:12] ) * beta[21]
  
  #Calculating XB
  XB = alpha + xbeta_p + xbeta_I
  
  #Calculating Numerator
  N = exp(XB)
  
  #Calculating Denominator
  D = exp(XB) %>% rowSums()
  
  #Calculating Probability p
  p_ij = N / D
  
  #Calculating log Likelihood
  log_lik = sum( y * log( p_ij ) )
    
  return(-log_lik)
}
```

Here, we optimize the log likelihood function.
```{r}
#Initial Values
betas = c(0, -1, 1, -2, -3, -2, 0.5, 1, 2, -4, 
         rep(0, 10), -6)

#Defining I
I = choice_I_df$Income

#Optimizing Log Likelihood Function
opt1 = optim(par = betas, fn = mixed_logit_log_lik)

opt_df = data.frame(
  c(opt1$par[1:10], opt1$par[11:20], opt1$par[21])
  )
colnames(opt_df) = c("Optimzed $\\hat{\\beta}^{f}$")
rownames(opt_df) = c("Intercept Reference",
                     paste("Intercept", 2:10, sep = ""), 
                     "Income Reference",
                     paste("Income", 2:10, sep = ""),
                           "Price")

kable(opt_df, digits = 4, caption = "Coefficients Under Mixed Logit")

```

Here, we also implement mixed logit log likelihood. But, this time, we compute the mixed logit log likelihood after removing choice 10.

```{r}
#Dataframe After Removing Choice 1
choicePrice2 = merged_data %>%
  filter(choice != 10)

n2 = nrow(choicePrice2)

#Allocating Memory for Matrix y
y2 = matrix(0, nrow = dim(choicePrice2)[1], 
           ncol = dim(choicePrice2[, 3:11])[2])

#Calculating y
for( j in seq(1, 10) ){
  for( i in 1:dim(choicePrice2)[1] ){
    
    if(choicePrice2$choice[i] == j){
        y2[i, j] = 1
    }
    
  }
}
  
#New Mixed Logit Log Likelihood
mixed_logit_log_lik2 = function(beta){
  
  #Intercept Matrix
  alpha = matrix( rep( beta[1:9], each = n2 ), nrow = n2, ncol = 9)
  
  #Non-Intercept Matrix
  xbeta_I = matrix( rep( beta[11:19], each = n2), nrow = n2, ncol = 9 ) *
    choicePrice2$Income
  xbeta_p = as.matrix( choicePrice2[, 3:11] ) * beta[21]
  
  #Calculating XB
  XB = alpha + xbeta_p + xbeta_I
  
  #Calculating Numerator
  N = exp(XB)
  
  #Calculating Denominator
  D = exp(XB) %>% rowSums()
  
  #Calculating Probability p
  p_ij = N / D
  
  #Calculating log Likelihood
  log_lik = sum( y2 * log( p_ij ) )
    
  return(-log_lik)
}
```

We optimize the removed mixed logit log likelihood.

```{r}
#Initial Values
betas = c(0, -1, 1, -2, -3, -2, 0.5, 1, 2, -4, 
         rep(0, 10), -6)

#Defining I
I = choice_I_df$Income

#Optimizing Log Likelihood Function
opt2 = optim(par = betas, fn = mixed_logit_log_lik2)

opt2_df = data.frame(
  c(opt2$par[1:9], opt2$par[11:19], opt2$par[21])
  )
colnames(opt2_df) = c("Optimzed $\\hat{\\beta}^{r}$")
rownames(opt2_df) = c("Intercept Reference",
                     paste("Intercept", 2:9, sep = ""), 
                     "Income Reference",
                     paste("Income", 2:9, sep = ""),
                           "Price")

kable(opt2_df, digits = 4, caption = "Coefficients Under Removed Mixed Logit")
```

We compute test statistics here. Note that the deviance follows a $\chi^{2}$ distribution.

```{r}
#Calculating Test Statistic
MTT = -2* (mixed_logit_log_lik2(opt1$par) - 
             mixed_logit_log_lik2(opt2$par))

#Table of Test Statistics
kable(data.frame(MTT), digits = 4, caption = "Test Stiatistics",
      col.names = "MTT")

#Acceptance / Rejection
pchisq(MTT, df = length(opt2$par[c(1:9, 11:19, 21)]), lower.tail = F)
```

## Conclusion

Since the test shows p-value of 1, IIA holds.
